{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMzxhaF9g09ub0yu73gv8pL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lov435/SOEmotions/blob/main/hugging_bert_goemotions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install PyTorch and BERT transformers from HuggingFace"
      ],
      "metadata": {
        "id": "eXmn46jGhVsE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kRaAG4yYPR-w",
        "outputId": "97adb1de-4650-4f35-d587-a03664b3e5fc"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.23.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.13.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.13.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.9.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.12.1+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (4.1.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import necessary packages"
      ],
      "metadata": {
        "id": "bhY6b8GUhdpO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch"
      ],
      "metadata": {
        "id": "qcCCIFMpbSf1"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Read the emotions prediction spreadsheet"
      ],
      "metadata": {
        "id": "JYm_1QGduCou"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url='https://drive.google.com/file/d/1OW1PZ-MvXFGd4KbqE8zjSakPNVKXXVLy/view?usp=sharing'\n",
        "file_id=url.split('/')[-2]\n",
        "dwn_url='https://drive.google.com/uc?id=' + file_id\n",
        "df = pd.read_csv(dwn_url)"
      ],
      "metadata": {
        "id": "HySoqY6CuBvy"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extract the Comment text and the corresponding Haoxiang's group from the spreadsheet. Also, remove the rows with empty comments"
      ],
      "metadata": {
        "id": "UoOMAUncv8gV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.dropna(axis=0, subset=['CommentTextProc'])\n",
        "#GoEmotions features columns\n",
        "goEmoCols = ['admiration', 'amusement', 'anger', 'annoyance', 'approval', 'caring', 'confusion', 'curiosity', 'desire', 'disappointment', 'disapproval', 'disgust', 'embarrassment', 'excitement', 'fear', 'gratitude', 'grief', 'joy', 'love', 'nervousness', 'optimism', 'pride', 'realization', 'relief', 'remorse', 'sadness', 'surprise', 'neutral'] \n",
        "X_cols = ['CommentTextProc']\n",
        "#Append the comment text to the goEmotions features to form a complete feature set\n",
        "X_cols.extend(goEmoCols)\n",
        "X = df[X_cols]\n",
        "#Group column is the class\n",
        "y = df['Group']"
      ],
      "metadata": {
        "id": "mPaXh3zav3D2"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Split the data into training and test"
      ],
      "metadata": {
        "id": "glPmJYZjrswr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
        "                                   random_state=104, \n",
        "                                   test_size=0.40, \n",
        "                                   shuffle=True)"
      ],
      "metadata": {
        "id": "r74s0FjGrsSN"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initialize the BERT tokenizer and model"
      ],
      "metadata": {
        "id": "6Zorts6qEIEj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained(\"bert-base-uncased\").to(device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uWUjF583iBrN",
        "outputId": "bce698a4-6e41-410b-b443-b3a680f39a22"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Produce the features (BERT vectors) from text"
      ],
      "metadata": {
        "id": "WuvdeK2oiFwA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#X_train, X_test, y_train, y_test\n",
        "encoded_x_train = tokenizer(list(X_train['CommentTextProc']), padding = True, truncation = True, return_tensors='pt')\n",
        "encoded_x_test = tokenizer(list(X_test['CommentTextProc']), padding = True, truncation = True, return_tensors='pt')\n",
        "\n",
        "#move on device (GPU)\n",
        "encoded_x_train = {k:torch.tensor(v).to(device) for k,v in encoded_x_train.items()}\n",
        "encoded_x_test = {k:torch.tensor(v).to(device) for k,v in encoded_x_test.items()}\n",
        "\n",
        "with torch.no_grad():\n",
        "  output_train = model(**encoded_x_train)\n",
        "  output_test = model(**encoded_x_test)\n",
        "\n",
        "#We need the [CLS] output for our classification task  \n",
        "cls_output_train = output_train.last_hidden_state[:,0,:]\n",
        "cls_output_test = output_test.last_hidden_state[:,0,:]\n",
        "\n",
        "print(\"Shape is\")\n",
        "print(cls_output_train.shape)\n",
        "print(cls_output_test.shape)"
      ],
      "metadata": {
        "id": "kdkWmm2IteIH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b29bf24-1164-449e-a531-e4dfb2bebd12"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  \n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  import sys\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape is\n",
            "torch.Size([2190, 768])\n",
            "torch.Size([1460, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Perform PCA"
      ],
      "metadata": {
        "id": "cADo4o-_2cdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use PCA to reduce dimensions from 768 to 10\n",
        "pca = PCA(n_components = 20, random_state = 7)\n",
        "#Temporarily concatenate the training and test features for PCA\n",
        "nump_features = np.concatenate((cls_output_train.detach().cpu().numpy(), cls_output_test.detach().cpu().numpy()))\n",
        "#print(nump_features.shape)\n",
        "#print(nump_features)\n",
        "X1 = pca.fit_transform(nump_features)\n",
        "print(X1.shape)\n",
        "print(X1)\n",
        "#Now slice the PCA feature set to training and test\n",
        "X_pca_train = X1[:len(y_train),]\n",
        "X_pca_test = X1[len(y_train):,]\n",
        "print(X_pca_train.shape)\n",
        "\n"
      ],
      "metadata": {
        "id": "DfwHnawtbyu_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58f96608-8079-471d-f10c-70867cb4d806"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3650, 20)\n",
            "[[ 3.4247313  -1.8072591   1.629929   ...  0.5630206  -0.5559147\n",
            "  -0.02455406]\n",
            " [ 0.24995746  4.0505266  -0.76881146 ... -0.04837902 -0.71838933\n",
            "   0.62327296]\n",
            " [ 1.352505    0.71638167 -0.01074351 ... -0.07519045  0.76614106\n",
            "   0.08123524]\n",
            " ...\n",
            " [ 1.590624   -2.0225754  -0.972284   ...  0.12262599  0.17904675\n",
            "  -0.21681839]\n",
            " [-1.9809672   1.7551185   0.5498128  ...  0.104182   -0.6239363\n",
            "   0.10105332]\n",
            " [-0.35026196  0.31541175 -0.04346808 ... -0.6486864   0.99373025\n",
            "   1.1642106 ]]\n",
            "(2190, 20)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Combine the PCA BERT features with the GoEmotions features"
      ],
      "metadata": {
        "id": "rdvRoAXrEVSD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_goEmo_train = X_train[goEmoCols].to_numpy()\n",
        "X_goEmo_test = X_test[goEmoCols].to_numpy()\n",
        "X_train_final = np.concatenate((X_goEmo_train, X_pca_train), axis=1)\n",
        "X_test_final = np.concatenate((X_goEmo_test, X_pca_test), axis=1)\n",
        "\n"
      ],
      "metadata": {
        "id": "GsgujFBqEa6W"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Perform a classification task"
      ],
      "metadata": {
        "id": "J58UMEm5NdJa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rf = RandomForestClassifier()\n",
        "rf.fit(X_train_final, y_train)\n",
        "rf.score(X_test_final, y_test)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oaCha_qwNhp3",
        "outputId": "e75229fd-1335-4d6c-d34c-7efba93b2364"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5910958904109589"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    }
  ]
}